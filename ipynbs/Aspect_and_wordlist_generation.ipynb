{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, ASCENDING\n",
    "from srs.database import connect_to_db\n",
    "from srs.utilities import Sentence, tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import math\n",
    "import word2vec\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "# Loading Word2Vec model\n",
    "current_directory = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "model_path = os.path.join(current_directory[:-6], 'srs/predictor_data/text8.bin')\n",
    "model = word2vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following functions accumulate all the sentences by category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_category_dict():\n",
    "    \"\"\"Build a dictionary whose key is the category tuple, and the value is a list of product_ids:\"\"\"\n",
    "    client, db = connect_to_db()\n",
    "    cursor = db.product_collection.find()\n",
    "    category_dict = {}\n",
    "    i = 0\n",
    "    for product in cursor:\n",
    "        i += 1   \n",
    "        if i % 100000 == 0:\n",
    "            print i\n",
    "        category = product['category']\n",
    "        category_short = tuple(category[:3]) #generally category is 4-tuple. Now limit to the first three tuple\n",
    "        product_id = product['product_id']\n",
    "\n",
    "        if category_short not in category_dict:\n",
    "            category_dict[category_short] = [product_id]\n",
    "        else:\n",
    "            category_dict[category_short].append(product_id)\n",
    "    client.close()\n",
    "    \n",
    "    return category_dict\n",
    "\n",
    "def sort_category_dict(category_dict, min_product_num, isPrint = 0):\n",
    "    \"\"\"Sort the categories according to the number of products in that category, and print them from top\"\"\"\n",
    "    category_list_sorted = []\n",
    "    category_list = []\n",
    "    for key in category_dict:\n",
    "        length = len(category_dict[key])\n",
    "        category_list.append([length, key])\n",
    "    category_list_sorted = sorted(category_list, key=lambda tup: tup[0], reverse=True)\n",
    "\n",
    "    if isPrint == 1:\n",
    "        for category_data in category_list_sorted:\n",
    "            if category_data[0] > min_product_num:\n",
    "                print category_data\n",
    "    return category_list_sorted\n",
    "\n",
    "def sort_list(list, sort_index, reverse = True):\n",
    "    list_sorted = sorted(list, key=lambda tup: tup[sort_index], reverse = reverse)\n",
    "    return list_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n"
     ]
    }
   ],
   "source": [
    "category_dict = get_category_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201512, (u'Cell Phones & Accessories', u'Cases', u'Basic Cases')]\n",
      "[54232, (u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories')]\n",
      "[50234, (u'Electronics', u'Computers & Accessories', u'Cables & Accessories')]\n",
      "[46040, (u'Electronics', u'Accessories & Supplies', u'Audio & Video Accessories')]\n",
      "[40292, (u'Electronics', u'Camera & Photo', u'Accessories')]\n",
      "[36801, (u'Electronics', u'Computers & Accessories', u'Touch Screen Tablet Accessories')]\n",
      "[26273, (u'Electronics', u'Computers & Accessories', u'Computer Components')]\n",
      "[25569, (u'Electronics', u'Portable Audio & Video', u'MP3 Players & Accessories')]\n",
      "[25256, (u'Cell Phones & Accessories', u'Accessories', u'Accessory Kits')]\n",
      "[16506, (u'Cell Phones & Accessories', u'Accessories', u'Chargers')]\n",
      "[15195, (u'Cell Phones & Accessories', u'Accessories', u'Screen Protectors')]\n",
      "[14752, (u'Electronics', u'Car & Vehicle Electronics', u'Car Electronics')]\n",
      "[11840, (u'Electronics', u'Computers & Accessories', u'Data Storage')]\n",
      "[11747, (u'Electronics', u'Camera & Photo', u'Bags & Cases')]\n",
      "[9669, (u'Cell Phones & Accessories', u'Accessories', u'Headsets')]\n",
      "[9326, (u'Cell Phones & Accessories', u'Accessories', u'Batteries')]\n",
      "[9150, (u'Electronics', u'Computers & Accessories', u'Laptops')]\n",
      "[9050, (u'Electronics', u'Car & Vehicle Electronics', u'Vehicle Electronics Accessories')]\n",
      "[8414, (u'Electronics', u'Camera & Photo', u'Lighting & Studio')]\n",
      "[8174, (u'Electronics', u'Home Audio', u'Stereo Components')]\n",
      "[7917, (u'Electronics', u'Camera & Photo', u'Digital Cameras')]\n",
      "[7585, (u'Electronics', u'Computers & Accessories', u'Networking Products')]\n",
      "[7472, (u'Electronics', u'Accessories & Supplies', u'Batteries, Chargers & Accessories')]\n",
      "[6351, (u'Cell Phones & Accessories', u'Accessories', u'Replacement Parts')]\n",
      "[6024, (u'Cell Phones & Accessories', u'Cell Phones', u'Unlocked Cell Phones')]\n",
      "[5953, (u'Cell Phones & Accessories', u'Accessories', u'Data Cables')]\n",
      "[5689, (u'Cell Phones & Accessories', u'Accessories', u'Car Accessories')]\n",
      "[5123, (u'Electronics', u'GPS & Navigation', u'GPS System Accessories')]\n",
      "[4485, (u'Electronics', u'Computers & Accessories')]\n",
      "[4227, (u'Electronics', u'Television & Video', u'Televisions')]\n",
      "[4081, (u'Cell Phones & Accessories', u'Cases', u'Holsters & Clips')]\n",
      "[4027, (u'Electronics', u'Accessories & Supplies', u'Telephone Accessories')]\n",
      "[3870, (u'Electronics', u'Computers & Accessories', u'Desktops')]\n",
      "[3812, (u'Electronics', u'Camera & Photo', u'Video Surveillance')]\n",
      "[3737, (u'Electronics', u'Camera & Photo', u'Lenses')]\n",
      "[3731, (u'Electronics', u'Computers & Accessories', u'Monitors')]\n",
      "[3694, (u'Electronics', u'Camera & Photo', u'Tripods & Monopods')]\n",
      "[3346, (u'Electronics', u'Camera & Photo', u'Binoculars & Scopes')]\n",
      "[3346, (u'Cell Phones & Accessories', u'Accessories', u'Stylus Pens')]\n",
      "[3346, (u'Electronics', u'eBook Readers & Accessories', u'Covers')]\n",
      "[3149, (u'Electronics', u'Camera & Photo', u'Video')]\n",
      "[3037, (u'Electronics', u'Computers & Accessories', u'PDAs, Handhelds & Accessories')]\n",
      "[2976, (u'Electronics', u'Portable Audio & Video', u'CB & Two-Way Radios')]\n",
      "[2944, (u'Cell Phones & Accessories', u'Accessories', u'Phone Charms')]\n",
      "[2467, (u'Electronics', u'Television & Video', u'DVD Players & Recorders')]\n",
      "[2439, (u'Electronics', u'Computers & Accessories', u'Tablets')]\n",
      "[2340, (u'Electronics', u'Accessories & Supplies', u'Blank Media')]\n",
      "[1832, (u'Electronics', u'Computers & Accessories', u'External Components')]\n",
      "[1665, (u'Electronics', u'Accessories & Supplies')]\n",
      "[1633, (u'Electronics', u'Camera & Photo', u'Film Photography')]\n",
      "[1559, (u'Electronics', u'Computers & Accessories', u'Routers')]\n",
      "[1543, (u'Electronics', u'Camera & Photo')]\n",
      "[1486, (u'Electronics', u'Computers & Accessories', u'Video Projectors')]\n",
      "[1459, (u'Electronics', u'Accessories & Supplies', u'Cord Management')]\n",
      "[1389, (u'Cell Phones & Accessories', u'Cases', u'Armbands')]\n",
      "[1347, (u'Cell Phones & Accessories', u'Accessories')]\n",
      "[1211, (u'Electronics', u'Security & Surveillance', u'Home Security Systems')]\n",
      "[1204, (u'Electronics', u'Camera & Photo', u'Flashes')]\n",
      "[1171, (u'Electronics', u'Portable Audio & Video', u'Radios')]\n",
      "[1134, (u'Electronics', u'Computers & Accessories', u'Webcams')]\n",
      "[1126, (u'Electronics', u'GPS & Navigation', u'Sports & Handheld GPS')]\n",
      "[1088, (u'Electronics', u'eBook Readers & Accessories', u'Skins')]\n",
      "[1005, (u'Electronics', u'Camera & Photo', u'Underwater Photography')]\n"
     ]
    }
   ],
   "source": [
    "# Shows all the main categories (up to 3rd level) and the number of product it contains:\n",
    "min_product_num = 1000\n",
    "category_list_sorted = sort_category_dict(category_dict, min_product_num, isPrint = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following function calculates a word's general tf-idf score from a category or an ensemble of categories, and save it to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentence_from_category(category):\n",
    "    \"\"\"Obtain all the review sentences from a category tuple:\"\"\"\n",
    "    client, db = connect_to_db()\n",
    "    product_id_list = category_dict[category]\n",
    "    sentence_list = []\n",
    "    review_num = 0\n",
    "    for product_id in product_id_list:\n",
    "        query_res = list(db.product_collection.find({\"product_id\": product_id}))\n",
    "        contents = query_res[0][\"contents\"]\n",
    "        sentence_list += contents\n",
    "        review_num += len(query_res[0][\"review_ids\"])\n",
    "    print \"Number of products {0}\\nNumber of reviews: {1}\\nNumber of sentences: {2}\".format(len(product_id_list), review_num, len(sentence_list))\n",
    "    client.close()\n",
    "    return sentence_list\n",
    "\n",
    "def get_sentence_from_category_ensemble(category_dict, max_prod_chosen = 500, min_product_level = 500):\n",
    "    client, db = connect_to_db()\n",
    "    full_sentence_list = []\n",
    "    print \"Getting product categories: (num_sentence_chosen, category):\"\n",
    "    for category in category_dict:\n",
    "        if len(category_dict[category]) < min_product_level:\n",
    "            continue\n",
    "        product_id_list = category_dict[category]\n",
    "        random.shuffle(product_id_list)\n",
    "        new_sentence = []\n",
    "        for product_id in product_id_list[:max_prod_chosen]:\n",
    "            query_res = list(db.product_collection.find({\"product_id\": product_id}))\n",
    "            contents = query_res[0][\"contents\"]\n",
    "            new_sentence += contents\n",
    "        print len(new_sentence),category\n",
    "        full_sentence_list += new_sentence\n",
    "    client.close()\n",
    "    print \"Number of sentences: {0}\".format(len(full_sentence_list))\n",
    "    return full_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_sentence_list = get_sentence_from_category_ensemble(category_dict, max_prod_chosen = 1000, min_product_level = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following functions collects sentences from one category, obtain each word's tf-idf score, and choose aspect candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tf_idf(sentence_list, is_idf_db = True):\n",
    "    \"\"\"Get tf-idf score for each word\n",
    "       The dictionary records for each word as a key, the [num_word, num_doc] value, where num_word means the number of \n",
    "       that word in the sentence_list, and num_doc means the number of sentences this word appears in.\n",
    "    \"\"\"\n",
    "    word_statistics = {}\n",
    "    db_word_score_list = db.word_score_list\n",
    "    i = 0\n",
    "    print \"Number of sentences processed:\"\n",
    "    # Getting each word's statistics: [num_word, num_doc]\n",
    "    for sentence in sentence_list:\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print i\n",
    "        tokens = tokenize(sentence, stem = False)\n",
    "        tokens_count = Counter(tokens)\n",
    "        for word in tokens_count:        \n",
    "            if word not in word_statistics:\n",
    "                word_statistics[word] = [tokens_count[word], 1]\n",
    "            else:\n",
    "                word_statistics[word][0] += tokens_count[word]\n",
    "                word_statistics[word][1] += 1\n",
    "      \n",
    "    total_num_doc = len(sentence_list)\n",
    "    word_tf_idf = []\n",
    "    \n",
    "    # Getting the maximum word frequency:\n",
    "    max_word_freq = 0\n",
    "    for word in word_statistics:\n",
    "        if word_statistics[word][0] > max_word_freq:\n",
    "            max_word_freq = word_statistics[word][0]\n",
    "    \n",
    "    # Getting the tf-idf score for each word\n",
    "    print \"Calculating tf-idf:\"\n",
    "    i = 0\n",
    "    for word in word_statistics:\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print i\n",
    "        tf = float(word_statistics[word][0]) / max_word_freq \n",
    "        # Calculating idf:     \n",
    "        num_doc = word_statistics[word][1]\n",
    "        idf_category = math.log(float(total_num_doc)/(0 + num_doc))\n",
    "        if is_idf_db == True:\n",
    "            word_score = get_word_score_from_db(word, db_word_score_list)\n",
    "            idf_db = word_score[2]\n",
    "        else:\n",
    "            idf_db = 1\n",
    "        word_tf_idf.append([word, tf * idf_db, tf, idf_category, idf_db])\n",
    "            \n",
    "    # Sorting the word_tf_idf list:\n",
    "    word_tf_idf = sorted(word_tf_idf, key=lambda tup: tup[1], reverse=True)\n",
    "    return word_tf_idf, word_statistics\n",
    "\n",
    "\n",
    "def save_word_score_to_db(word_tf_idf):\n",
    "    client, db = connect_to_db()\n",
    "    db.word_score_list.delete_many({})\n",
    "    db.word_score_list.create_index([(\"word\", ASCENDING)])\n",
    "    i = 0\n",
    "    for word_data in word_tf_idf:\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print i\n",
    "        word = word_data[0]\n",
    "        word_score = word_data[1:]\n",
    "        query = {\"word\": word}\n",
    "        update_field = {\"word_score\": word_score}\n",
    "        \n",
    "        db.word_score_list.update_one(query, {\"$set\": update_field}, True)\n",
    "    client.close()\n",
    "    print \"Total number of words: %g\"%len(word_tf_idf)\n",
    "\n",
    "    \n",
    "def get_word_score_from_db(word, db_word_score_list):\n",
    "    query = db_word_score_list.find({\"word\": word})\n",
    "    if len(query[0]) > 0:\n",
    "        return query[0][\"word_score\"]\n",
    "    else:\n",
    "        return [0,0,0]\n",
    "\n",
    "\n",
    "def get_aspect_cadidate(word_tf_idf, tag_list = [\"NN\"], score_threshold = 0.8):\n",
    "    '''Get cadidate aspects from word_tf_idf. Only words whose tag belong to tag_list and score > threshold will pass'''\n",
    "    aspect_cadidate = []\n",
    "    j = 0\n",
    "    for word_data in word_tf_idf:\n",
    "        word = word_data[0]\n",
    "        tf_idf = word_data[1]\n",
    "        word_tag = pos_tag([word])[0][1]\n",
    "        # If any of the tag string (e.g. \"NN\") in the given tag_list appears in the word's tag (e.g. \"NNS\")\n",
    "        if any(tag in word_tag for tag in tag_list) and (word_data[1] >= score_threshold \\\n",
    "                                                         or (word_data[1] < score_threshold and j <= 20)):\n",
    "            j +=1\n",
    "            aspect_cadidate_data = [word, tf_idf, word_tag]\n",
    "            aspect_cadidate.append(aspect_cadidate_data)\n",
    "            print [word, '%0.2f' % tf_idf, word_tag]\n",
    "        if word_data[1] < score_threshold:\n",
    "            break\n",
    "            \n",
    "    return aspect_cadidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products 743\n",
      "Number of reviews: 32200\n",
      "Number of sentences: 221681\n"
     ]
    }
   ],
   "source": [
    "# Sample categories, with decreasing number of sentences\n",
    "# category = (\"Electronics\", \"Camera & Photo\", \"Digital Cameras\") #Control Group: 7916 products, 203836 reviews, 1724928 sentences\n",
    "# category = (u'Electronics', u'Computers & Accessories', u'Tablets') #2439 products, 99311 reviews, 781032 sentences\n",
    "category = (u'Cell Phones & Accessories', u'Cell Phones', u'No-Contract Cell Phones') # 743 products, 32200 reviews, 221681 sentences\n",
    "# category = (u'Cell Phones & Accessories', u'Accessories', u'Bluetooth Speakers') #609 products, 38847 reviews, 233048 sentences\n",
    "# category = (u'Electronics', u'Portable Audio & Video', u'Portable DVD Players') #222 products, 3977 reviews, 22476 sentences\n",
    "sentence_list = get_sentence_from_category(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences processed:\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "Calculating tf-idf:\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "word_tf_idf, _ = get_tf_idf(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'phone', '3.07', 'NN']\n",
      "[u'minutes', '0.53', 'NNS']\n",
      "[u'tracfone', '0.52', 'NN']\n",
      "[u'use', '0.46', 'NN']\n",
      "[u'phones', '0.45', 'NNS']\n",
      "[u'service', '0.44', 'NN']\n",
      "[u'battery', '0.40', 'NN']\n",
      "[u'screen', '0.39', 'NN']\n",
      "[u\"it's\", '0.36', 'NN']\n",
      "[u'text', '0.35', 'NN']\n",
      "[u'apps', '0.35', 'NN']\n",
      "[u'card', '0.33', 'NN']\n",
      "[u'call', '0.32', 'NN']\n",
      "[u'time', '0.31', 'NN']\n",
      "[u\"don't\", '0.30', 'NN']\n",
      "[u'calls', '0.30', 'NNS']\n",
      "[u'lg', '0.28', 'NN']\n",
      "[u'android', '0.28', 'NN']\n",
      "[u'virgin', '0.28', 'NN']\n",
      "[u'mobile', '0.26', 'NN']\n",
      "[u'g', '0.26', 'NN']\n",
      "[u\"i'm\", '0.24', 'NN']\n",
      "[u'plan', '0.24', 'NN']\n",
      "[u'cell', '0.24', 'NN']\n",
      "[u'price', '0.23', 'NN']\n",
      "[u'bought', '0.23', 'NN']\n",
      "[u'love', '0.22', 'NN']\n",
      "[u'life', '0.22', 'NN']\n",
      "[u'month', '0.22', 'NN']\n",
      "[u\"i've\", '0.22', 'NN']\n",
      "[u'want', '0.22', 'NN']\n",
      "[u'works', '0.22', 'NNS']\n",
      "[u'need', '0.22', 'NN']\n",
      "[u'keyboard', '0.21', 'NN']\n",
      "[u'work', '0.21', 'NN']\n",
      "[u'data', '0.21', 'NNS']\n",
      "[u'number', '0.21', 'NN']\n",
      "[u'camera', '0.21', 'NN']\n"
     ]
    }
   ],
   "source": [
    "tag_list = [\"NN\"]  # Include in VB since some nouns are mis-classified as verbs\n",
    "aspect_candidate_list = get_aspect_cadidate(word_tf_idf, tag_list, score_threshold = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"phone\", \"minutes\", \"tracfone\", \"use\", \"phones\", \"service\", \"battery\", \"screen\", \"it's\", \"text\", \"apps\", \"card\", \"call\", \"time\", \"don't\", \"calls\", \"lg\", \"android\", \"virgin\", \"mobile\", \"g\", \"i'm\", \"plan\", \"cell\", \"price\", \"bought\", \"love\", \"life\", \"month\", \"i've\", \"want\", \"works\", \"need\", \"keyboard\", \"work\", \"data\", \"number\", \"camera\",\n"
     ]
    }
   ],
   "source": [
    "for item in aspect_candidate_list:\n",
    "    print '\"%s\",'%item[0],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following functions start from a seed_word list, find the word list that serves as a dict for each seed_word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similarity(word1, word2):\n",
    "    \"\"\"Find the similarity between two words, which equals the dot product of their vectors\"\"\"\n",
    "    similarity = 0\n",
    "    word1=word1.lower()\n",
    "    word2=word2.lower()\n",
    "    if word1 in model and word2 in model:\n",
    "        word1_vec = model[word1]\n",
    "        word2_vec = model[word2]\n",
    "        similarity = np.dot(word1_vec, word2_vec)\n",
    "    return similarity\n",
    "\n",
    "def get_word_list_from_aspect_candidates(seed_word, word_tf_idf, similarity_threshold, score_threshold):\n",
    "    \"\"\"Method 1: directly find the word list from all words whose similarity with the seed_word and tf-idf score are above \n",
    "    certain threshold\"\"\"\n",
    "    word_list = []\n",
    "    for word_data in word_tf_idf:\n",
    "        word = word_data[0]\n",
    "        tf_idf = word_data[1]\n",
    "        if tf_idf > score_threshold:\n",
    "            similarity = get_similarity(seed_word, word)\n",
    "            if similarity > similarity_threshold:\n",
    "                word_list.append([word, similarity, tf_idf])              \n",
    "    word_list_sorted = sorted(word_list, key=lambda tup: tup[1], reverse=True)\n",
    "    return word_list_sorted\n",
    "\n",
    "\n",
    "def get_word_list_by_tf_idf(seed_word_list, sentence_list, num_words_in_list, sim_slope = 0.5, sim_intercept = 0.2):\n",
    "    \"\"\"Method 2: Find the word_list who can distinguish the chosen sentences from other sentences\"\"\"\n",
    "    \n",
    "    # For each seed_word in seed_word_list, get the word_data for all sentences. word_data has 4 fields [num_word_total, num_doc_total, \n",
    "    # num_word_in_topic, num_doc_in_topic, similarity with seed_word], the first two are from all sentences, and the latter two are from the sentences \n",
    "    # that contain the seed_word.   \n",
    "    num_seed_word = len(seed_word_list)\n",
    "    word_statistics_dic_list = [{} for i in range(num_seed_word)]\n",
    "    \n",
    "    num_sentence_topic_list = [0 for k in range(num_seed_word)]\n",
    "    num_sentence_total_list = [0 for k in range(num_seed_word)]\n",
    "    i = 0\n",
    "#     print \"Number of sentences processed:\"\n",
    "    for sentence in sentence_list: \n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print i\n",
    "        tokens = tokenize(sentence, stem = False)\n",
    "        tokens_count = Counter(tokens)\n",
    "        for word in tokens_count:\n",
    "            # check for each seed_word:\n",
    "            for k in range(num_seed_word):\n",
    "                seed_word = seed_word_list[k]\n",
    "                if seed_word in sentence: \n",
    "                    num_sentence_topic_list[k] += 1\n",
    "                    num_sentence_total_list[k] += 1\n",
    "                    if word not in word_statistics_dic_list[k]:\n",
    "                        word_statistics_dic_list[k][word] = [tokens_count[word], 1, tokens_count[word], 1]\n",
    "                    else:\n",
    "                        word_statistics_dic_list[k][word][0] += tokens_count[word]\n",
    "                        word_statistics_dic_list[k][word][1] += 1\n",
    "                        word_statistics_dic_list[k][word][2] += tokens_count[word]\n",
    "                        word_statistics_dic_list[k][word][3] += 1\n",
    "                else:\n",
    "                    num_sentence_total_list[k] += 1\n",
    "                    if word not in word_statistics_dic_list[k]:\n",
    "                        word_statistics_dic_list[k][word] = [tokens_count[word], 1, 0, 0]\n",
    "                    else:\n",
    "                        word_statistics_dic_list[k][word][0] += tokens_count[word]\n",
    "                        word_statistics_dic_list[k][word][1] += 1\n",
    "\n",
    "    # Get the maximum word frequency for each seed_word group:\n",
    "    word_tf_idf_ratio_list = [[] for k in range(num_seed_word)]\n",
    "    max_num_word_total_list =[0 for k in range(num_seed_word)]\n",
    "    max_num_word_topic_list =[0 for k in range(num_seed_word)] \n",
    "    for k in range(num_seed_word):    \n",
    "        for word in word_statistics_dic_list[k]:\n",
    "            word_data = word_statistics_dic_list[k][word]\n",
    "            if word_data[0] > max_num_word_total_list[k]:\n",
    "                max_num_word_total_list[k] = word_data[0]\n",
    "            if word_data[2] > max_num_word_topic_list[k]:\n",
    "                max_num_word_topic_list[k] = word_data[2]\n",
    "    \n",
    "    # Get tf_idf adjusted ratio for each word, to measure how this word can distinguish the topic sentences:\n",
    "    for k in range(num_seed_word):        \n",
    "        for word in word_statistics_dic_list[k]:\n",
    "            word_data = word_statistics_dic_list[k][word]\n",
    "            num_word_total = word_data[0]\n",
    "            num_word_topic = word_data[2]\n",
    "            num_doc_total = word_data[1]   \n",
    "            num_doc_topic = word_data[3]\n",
    "        \n",
    "            if num_doc_topic == 0 or num_doc_total == 0:\n",
    "                word_tf_idf_ratio_list[k].append([word, 0, 0, 0])\n",
    "                continue\n",
    "\n",
    "            tf_topic = float(num_word_topic) / max_num_word_topic_list[k]\n",
    "            tf_total = float(num_word_total) / max_num_word_total_list[k]\n",
    "            tf_ratio = (tf_topic/num_word_topic) / (tf_total/num_doc_total)\n",
    "\n",
    "            idf_topic = math.log(float(num_sentence_topic_list[k]) / num_doc_topic)\n",
    "            idf_total = math.log(float(num_sentence_total_list[k]) / num_doc_total) \n",
    "\n",
    "            word_tf_idf_ratio_list[k].append([word, 0, 0, tf_topic * math.log(tf_ratio) * idf_total ** 2, tf_topic, tf_total, tf_ratio, idf_topic, idf_total])\n",
    "\n",
    "        word_tf_idf_ratio_list[k].sort(key=lambda tup: tup[3], reverse=True)\n",
    " \n",
    "    for k in range(num_seed_word): \n",
    "        for j in range(100):\n",
    "            word_data = word_tf_idf_ratio_list[k][j]\n",
    "            word = word_data[0]\n",
    "            similarity = get_similarity(word, seed_word_list[k])\n",
    "            sim_amplify = sim_intercept + similarity * sim_slope\n",
    "            word_tf_idf_ratio_list[k][j][2] = sim_amplify\n",
    "            argument = 1 + word_tf_idf_ratio_list[k][j][3] * sim_amplify\n",
    "            if argument > 0:\n",
    "                word_tf_idf_ratio_list[k][j][1] = math.log(argument) \n",
    "            else:\n",
    "                word_tf_idf_ratio_list[k][j][1] = -1\n",
    "        word_tf_idf_ratio_list[k].sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    word_tf_idf_ratio_list = [item[:num_words_in_list] for item in word_tf_idf_ratio_list]\n",
    "    word_list = {}\n",
    "    for k in range(num_seed_word):\n",
    "        word_list[seed_word_list[k]] = [[word_data[0],word_data[1]] for word_data in word_tf_idf_ratio_list[k][:num_words_in_list]]\n",
    "    return word_list, word_tf_idf_ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "\"service\": \n",
      "   \"service\", 4.37;  \"customer\", 2.67;  \"services\", 2.01;  \"phone\", 1.90;  \"minutes\", 1.60;  \"mobile\", 1.52;  \"month\", 1.31;  \"net\", 1.28;  \"days\", 1.27;  \"year\", 1.20; \n",
      "\"battery\": \n",
      "   \"battery\", 4.35;  \"charge\", 1.83;  \"phone\", 1.83;  \"life\", 1.63;  \"hours\", 1.31;  \"drain\", 1.30;  \"use\", 1.29;  \"remove\", 1.26;  \"screen\", 1.23;  \"fast\", 1.17; \n",
      "\"text\": \n",
      "   \"text\", 4.75;  \"texts\", 3.57;  \"messages\", 2.75;  \"message\", 2.68;  \"texting\", 2.57;  \"phone\", 2.46;  \"data\", 2.33;  \"keyboard\", 2.28;  \"calls\", 2.02;  \"web\", 2.00; \n",
      "\"screen\": \n",
      "   \"screen\", 4.30;  \"touch\", 2.76;  \"touchscreen\", 2.05;  \"phone\", 1.96;  \"screens\", 1.94;  \"keyboard\", 1.90;  \"camera\", 1.55;  \"button\", 1.51;  \"buttons\", 1.33;  \"size\", 1.29; \n",
      "\"apps\": \n",
      "   \"apps\", 4.64;  \"download\", 2.29;  \"phone\", 2.15;  \"memory\", 2.13;  \"app\", 2.01;  \"sd\", 2.00;  \"google\", 1.99;  \"card\", 1.83;  \"install\", 1.80;  \"storage\", 1.64; \n",
      "\"camera\": \n",
      "   \"camera\", 4.77;  \"video\", 2.57;  \"flash\", 2.49;  \"phone\", 2.17;  \"cameras\", 2.14;  \"front\", 2.11;  \"mp\", 2.09;  \"pictures\", 2.09;  \"screen\", 2.06;  \"quality\", 1.85; \n",
      "\"call\": \n",
      "   \"call\", 4.18;  \"calls\", 3.71;  \"calling\", 2.77;  \"called\", 2.67;  \"phone\", 2.63;  \"make\", 2.49;  \"get\", 2.06;  \"automatically\", 1.90;  \"use\", 1.82;  \"hear\", 1.79; \n",
      "\"plan\": \n",
      "   \"plan\", 4.94;  \"plans\", 3.59;  \"data\", 2.70;  \"month\", 2.29;  \"contract\", 2.12;  \"pay\", 2.06;  \"unlimited\", 2.02;  \"phone\", 2.00;  \"service\", 1.90;  \"monthly\", 1.87; \n",
      "\"keyboard\": \n",
      "   \"keyboard\", 5.00;  \"qwerty\", 3.32;  \"slide\", 2.73;  \"screen\", 2.51;  \"phone\", 2.31;  \"keys\", 2.26;  \"keyboards\", 2.26;  \"typing\", 1.98;  \"use\", 1.91;  \"touchscreen\", 1.89; \n",
      "\"data\": \n",
      "   \"data\", 5.03;  \"plan\", 2.70;  \"unlimited\", 2.58;  \"phone\", 2.35;  \"wifi\", 2.27;  \"text\", 2.24;  \"mb\", 1.97;  \"minutes\", 1.95;  \"use\", 1.90;  \"month\", 1.85; \n",
      "\"price\": \n",
      "   \"price\", 4.67;  \"prices\", 2.33;  \"good\", 2.08;  \"phone\", 2.04;  \"priced\", 1.93;  \"low\", 1.67;  \"worth\", 1.55;  \"better\", 1.52;  \"great\", 1.48;  \"paid\", 1.43; \n",
      "\"card\": \n",
      "   \"card\", 4.62;  \"sd\", 2.86;  \"cards\", 2.85;  \"sim\", 2.81;  \"gb\", 2.49;  \"memory\", 2.40;  \"phone\", 2.39;  \"micro\", 2.09;  \"slot\", 1.91;  \"minutes\", 1.90; \n"
     ]
    }
   ],
   "source": [
    "seed_word_list = [\"battery\",\"pictures\",\"price\",\"zoom\",\"ease of use\",\"detection\",\"design\",\"video\",\"quality\",\"screen\",\"size\"] # Camera\n",
    "seed_word_list = ['screen', \"battery\", \"price\", \"keyboard\", \"wifi\", \"games\", \"touch\", \"quality\",\"camera\",\"video\"] # Tablets\n",
    "seed_word_list = [\"service\", \"battery\", \"screen\", \"text\", \"apps\", \"card\", \"call\", \"plan\", \"price\", \"keyboard\",\"data\", \"camera\"] # no contract phones\n",
    "sim_slope = 1\n",
    "sim_intercept = 0.2\n",
    "wordlist_dict, _ = get_word_list_by_tf_idf(seed_word_list, sentence_list, 10, sim_slope, sim_intercept)\n",
    "num_seed_word = len(word_list)\n",
    "for aspect in word_list:\n",
    "    print '\"{0}\": '.format(aspect)\n",
    "    print '  ',\n",
    "    for word_data in word_list[aspect]:\n",
    "        print '\"%s\", %0.2f; '%(word_data[0], word_data[1]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeWordlistDictToDB(category, wordlist_dict):\n",
    "    client, db = connect_to_db()\n",
    "    query = {\"category\": list(category)}\n",
    "    update_field = {\"wordlist_dict\": wordlist_dict}\n",
    "    db.category_collection.update_one(query, {\"$set\": update_field}, True)\n",
    "    client.close()\n",
    "\n",
    "def getWordlistDictFromDB(category):\n",
    "    client, db = connect_to_db()\n",
    "    category_collection = db.category_collection\n",
    "    query_res = list(category_collection.find({\"category\": category}))\n",
    "    disconnect_db(client)\n",
    "\n",
    "    if len(query_res) < 1:\n",
    "        raise Exception('Category: {0} not found in database'.format(category))\n",
    "    elif len(query_res) > 1:\n",
    "        raise Exception('Category: {0} found multiple occurances in database'.format(category))\n",
    "\n",
    "    result = query_res[0]\n",
    "    wordlistDictWithWeights = result['wordlist_dict']\n",
    "    wordlistDict = {}\n",
    "    for aspect in wordlistDictWithWeights:\n",
    "        wordlistDict[aspect] = [sublist[0] for sublist in wordlistDictWithWeights[aspect]]\n",
    "\n",
    "    return wordlistDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category = (u'Cell Phones & Accessories', u'Cell Phones', u'No-Contract Cell Phones')\n",
    "writeWordlistDictToDB(category, wordlist_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
